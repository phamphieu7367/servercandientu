{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd213f89",
   "metadata": {},
   "source": [
    "# Fine-tune LLaMA‑3.1 8B (QLoRA) trên Google Colab (T4)\n",
    "\n",
    "**Phiên bản:** file hướng dẫn đầy đủ, tối ưu cho GPU T4 (16 GB VRAM).\n",
    "\n",
    "**Ghi chú quan trọng:**\n",
    "- LLaMA‑8B yêu cầu cẩn trọng trên T4. Script này sử dụng QLoRA, bitsandbytes và offload để giảm VRAM.\n",
    "- Bạn *nên* đổi sang A100 nếu có khối lượng dataset lớn (>10k) hoặc muốn huấn luyện nhanh hơn.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a83d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Cài đặt thư viện cần thiết\n",
    "# Phiên bản được chọn tương thích với QLoRA + TRL + PEFT\n",
    "!pip install -q -U \"transformers>=4.40.0\" accelerate bitsandbytes datasets peft trl sentencepiece safetensors huggingface_hub\n",
    "\n",
    "# Kiểm tra GPU\n",
    "import torch, os\n",
    "print('Torch version:', torch.__version__)\n",
    "if torch.cuda.is_available():\n",
    "    print('CUDA device:', torch.cuda.get_device_name(0))\n",
    "    try:\n",
    "        print('CUDA memory (GB):', round(torch.cuda.get_device_properties(0).total_memory/1024**3,1))\n",
    "    except:\n",
    "        pass\n",
    "else:\n",
    "    print('CUDA not available - Colab runtime must have GPU enabled (Runtime > Change runtime type > GPU)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613697b2",
   "metadata": {},
   "source": [
    "## 2) Đăng nhập HuggingFace\n",
    "\n",
    "Bạn cần HuggingFace token để tải model `meta-llama/Llama-3.1-8b-instruct`. Tạo token tại: https://huggingface.co/settings/tokens\n",
    "\n",
    "Chạy cell dưới và dán token khi được hỏi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf20f14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "# Chạy lệnh và dán token vào prompt\n",
    "login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97447dc1",
   "metadata": {},
   "source": [
    "## 3) Chuẩn bị dataset\n",
    "\n",
    "- Nếu bạn chưa có dataset, cell bên dưới sẽ tạo một **dataset mẫu nhỏ** (100 mẫu) để thử nghiệm pipeline.\n",
    "- Khi bạn có dataset thật, upload file `train.jsonl` vào `/content/data/train.jsonl` (mỗi dòng một JSON: `{\"instruction\":..., \"input\":..., \"output\":...}`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c2738d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tạo folder data và dataset mẫu 100 mẫu để test\n",
    "import os, json, random\n",
    "os.makedirs('/content/data', exist_ok=True)\n",
    "def make_sample(i):\n",
    "    instr = \"Phân tích kết quả học tập học kỳ 1 và viết nhận xét ngắn 3 câu.\"\n",
    "    sample_input = {\"Toan\": round(random.uniform(4,10),1),\n",
    "                    \"Ly\": round(random.uniform(3,10),1),\n",
    "                    \"Van\": round(random.uniform(3,10),1),\n",
    "                    \"Anh\": round(random.uniform(3,10),1)}\n",
    "    out = f\"Học sinh có điểm Toán {sample_input['Toan']}, Lý {sample_input['Ly']}, Văn {sample_input['Van']}, Anh {sample_input['Anh']}. Nhận xét: ...\"\n",
    "    return {\"instruction\": instr, \"input\": json.dumps(sample_input, ensure_ascii=False), \"output\": out}\n",
    "\n",
    "with open('/content/data/train.jsonl','w', encoding='utf-8') as f:\n",
    "    for i in range(100):\n",
    "        f.write(json.dumps(make_sample(i), ensure_ascii=False) + '\\n')\n",
    "\n",
    "print('Tạo file /content/data/train.jsonl (100 mẫu) - Replace bằng dataset thật khi có.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6fb869",
   "metadata": {},
   "source": [
    "## 4) Load model & tokenizer (QLoRA-friendly)\n",
    "\n",
    "**Chú ý:** model tên `meta-llama/Llama-3.1-8b-instruct` được sử dụng. Cell này thiết lập load 4bit và device_map thích hợp với Colab T4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2fe890",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "model_name = \"meta-llama/Llama-3.1-8b-instruct\"\n",
    "\n",
    "print('Loading tokenizer...')\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False, trust_remote_code=True)\n",
    "\n",
    "print('Loading model in 4-bit (may take a few minutes)...')\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    load_in_4bit=True,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "print('Done loading model (4-bit).')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11c9e12",
   "metadata": {},
   "source": [
    "## 5) Cấu hình QLoRA (PEFT/LoRA)\n",
    "\n",
    "Tối ưu cấu hình LoRA dành cho T4: sử dụng r nhỏ, dropout vừa phải. Bạn có thể điều chỉnh `r` hoặc `lora_alpha` nếu cần."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f35622",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\",\"v_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "print('Applying LoRA...')\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be00435",
   "metadata": {},
   "source": [
    "## 6) Chuẩn bị dataset cho Trainer\n",
    "\n",
    "Sử dụng `datasets` để load file JSONL đã upload (hoặc mẫu). Chúng ta sẽ tạo trường `text` chứa prompt đầy đủ (Instruction + Input + Response)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd8105b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data_path = '/content/data/train.jsonl'\n",
    "ds = load_dataset('json', data_files=data_path)['train']\n",
    "\n",
    "def to_text(example):\n",
    "    instr = example.get('instruction','')\n",
    "    inp = example.get('input','')\n",
    "    out = example.get('output','')\n",
    "    if inp:\n",
    "        txt = f\"### Instruction:\\n{instr}\\n\\n### Input:\\n{inp}\\n\\n### Response:\\n{out}\"\n",
    "    else:\n",
    "        txt = f\"### Instruction:\\n{instr}\\n\\n### Response:\\n{out}\"\n",
    "    return {'text': txt}\n",
    "\n",
    "ds = ds.map(to_text)\n",
    "print('Dataset size:', len(ds))\n",
    "print(ds[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c55de3",
   "metadata": {},
   "source": [
    "## 7) Cấu hình Trainer & Huấn luyện\n",
    "\n",
    "Thiết lập phù hợp cho T4: very small per-device batch size + gradient accumulation. Điều chỉnh `num_train_epochs` theo dataset của bạn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9991f838",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./llama_8b_qlora_output',\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=32,\n",
    "    num_train_epochs=2,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,\n",
    "    logging_steps=20,\n",
    "    save_steps=200,\n",
    "    save_total_limit=2,\n",
    "    optim='paged_adamw_8bit',\n",
    "    remove_unused_columns=False\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=ds,\n",
    "    dataset_text_field='text',\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_length=1024,\n",
    "    packing=False\n",
    ")\n",
    "\n",
    "print('Ready to train. To start training, run: trainer.train()')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d255f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8) BẮT ĐẦU HUẤN LUYỆN\n",
    "# Bật cell này nếu bạn đã kiểm tra mọi thứ và muốn bắt đầu training.\n",
    "# LƯU Ý: training có thể tốn thời gian; với dataset mẫu 100 mẫu, nó sẽ hoàn thành nhanh.\n",
    "# Nếu runtime bị out-of-memory, giảm gradient_accumulation_steps hoặc max_seq_length.\n",
    "\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225d0267",
   "metadata": {},
   "source": [
    "## 9) Lưu LoRA adapters & tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0958433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sau khi huấn luyện xong, lưu LoRA adapters và tokenizer\n",
    "out_dir = '/content/lora-output'\n",
    "model.save_pretrained(out_dir)\n",
    "tokenizer.save_pretrained(out_dir)\n",
    "print('Saved LoRA adapters to', out_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c94d937",
   "metadata": {},
   "source": [
    "## 10) (Tùy chọn) Merge LoRA thành model hoàn chỉnh\n",
    "\n",
    "Nếu muốn xuất model hoàn chỉnh để deploy, bạn có thể merge adapters (chú ý VRAM lớn khi merge)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a31c33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from peft import AutoPeftModelForCausalLM\n",
    "# merged = AutoPeftModelForCausalLM.from_pretrained(out_dir, device_map='auto')\n",
    "# merged = merged.merge_and_unload()\n",
    "# merged.save_pretrained('/content/llama-8b-merged')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c99868",
   "metadata": {},
   "source": [
    "## 11) Kiểm tra inference (sau khi lưu LoRA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74b9987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kiểm tra inference nhanh (nếu LoRA đã được load)\n",
    "from transformers import pipeline\n",
    "try:\n",
    "    gen = pipeline('text-generation', model=model, tokenizer=tokenizer, device_map='auto')\n",
    "    prompt = \"### Instruction:\\nPhân tích ngắn kết quả học kỳ cho học sinh: Toán 8.7, Lý 5, Văn 7.\\n\\n### Response:\\n\"\n",
    "    out = gen(prompt, max_new_tokens=200, do_sample=False)[0]['generated_text']\n",
    "    print(out)\n",
    "except Exception as e:\n",
    "    print('Inference test failed:', e)\n",
    "    print('If inference fails on-colab, try loading the LoRA adapters via the saved directory and re-run.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0ecb9e",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "# GHI CHÚ CUỐI\n",
    "- Thay `data/train.jsonl` bằng dataset thật của bạn (3.000–5.000 mẫu tối thiểu để bắt đầu).\n",
    "- Nếu gặp OOM: giảm `gradient_accumulation_steps`, `max_seq_length`, hoặc chuyển sang GPU lớn hơn (A100).\n",
    "- Muốn mình tạo dataset synthetic 3.000 mẫu chuẩn instruct? Trả lời 'Có' và mình sẽ tạo luôn file dataset cho bạn.\n",
    "\n",
    "Chúc bạn thành công!"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
