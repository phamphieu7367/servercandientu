{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fine-tuning LLaMA 3.1 8B Instruct on Google Colab\n",
        "\n",
        "This notebook installs required libraries, loads the base model, prepares data, and runs LoRA fine-tuning using **Unsloth** for efficiency.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "!pip install unsloth transformers accelerate bitsandbytes datasets trl"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "model_name = 'meta-llama/Llama-3.1-8b-instruct'\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name,\n",
        "    load_in_4bit=True,\n",
        ")\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=16,\n",
        "    target_modules=['q_proj','k_proj','v_proj','o_proj'],\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.05,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Example dataset â€” user should replace with their own\n",
        "dataset = load_dataset('json', data_files='train.json')\n",
        "\n",
        "def format(example):\n",
        "    return {\n",
        "        'text': f\"<s>[INST] {example['instruction']} [/INST] {example['output']} </s>\"\n",
        "    }\n",
        "\n",
        "dataset = dataset.map(format)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='llama-3.1-8b-finetune',\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=8,\n",
        "    warmup_steps=5,\n",
        "    num_train_epochs=1,\n",
        "    learning_rate=2e-4,\n",
        "    fp16=True,\n",
        "    logging_steps=20,\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=dataset['train'],\n",
        "    dataset_text_field='text',\n",
        "    args=training_args,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model.save_pretrained('llama-finetuned')\n",
        "tokenizer.save_pretrained('llama-finetuned')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}